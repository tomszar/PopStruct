{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "This script will run a quality control pipeline in the genotype files.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "First, let's import modules and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, shutil, subprocess\n",
    "import pandas as pd\n",
    "from GenotypeQC import QC_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpath  = os.path.realpath(\"..\")\n",
    "pathgenos = os.path.join(projpath, \"DataBases\", \"Genotypes\")\n",
    "pathdat   = os.path.join(projpath, \"DataBases\")\n",
    "outpath   = os.path.join(pathgenos, \"02_Clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain common IDs\n",
    "\n",
    "For each dataset, we will retain only the common IDs.\n",
    "First, we will need to extract the FID for each IID, from the fam files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading common IDS\n",
    "os.chdir(pathdat)\n",
    "common_ids = pd.read_csv(\"common_ids.txt\", header=None, names = [\"ID\"])\n",
    "\n",
    "#Creating empty dataframe\n",
    "common_FID_ID = pd.DataFrame(columns=[\"FID\", \"ID\"])\n",
    "\n",
    "#Reading fam files and adding to empty dataframe\n",
    "os.chdir(os.path.join(pathgenos, \"01_Original\"))\n",
    "for fam in glob.glob(\"*.fam\"):\n",
    "    #UIUC2014 are repeated\n",
    "    if fam == \"UIUC2014_168ppl_703K_hg19_ATGC.fam\":\n",
    "        pass\n",
    "    else:\n",
    "        famfile = pd.read_csv(fam, header=None, sep=\" \", names=[\"FID\", \"ID\"], usecols=[\"FID\", \"ID\"], dtype=object )\n",
    "        inter = pd.merge(common_ids, famfile, on=\"ID\")[ [\"FID\", \"ID\"]]\n",
    "        common_FID_ID = pd.concat([common_FID_ID, inter], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will save the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(pathdat)\n",
    "common_FID_ID.to_csv(\"common_fid_id.txt\", sep=\" \", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will extract for each dataset only the common IDs, in map ped file for the lift over process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(pathgenos, \"01_Original\"))\n",
    "pathsave_commonid = os.path.join(pathgenos, \"01_Original\", \"Extract_Common_ID\")\n",
    "keepfile = os.path.join(pathdat, \"common_fid_id.txt\")\n",
    "\n",
    "for bedfile in glob.glob(\"*.bed\"):\n",
    "    filename = bedfile.split(\".\")[0]\n",
    "    outfile  = filename + \"_CID\"\n",
    "    if \"CV_\" in filename or \"Euro180_\" in filename:\n",
    "        subprocess.run([\"plink\", \"--bfile\", filename, \"--keep\", keepfile, \"--recode\", \"--out\", os.path.join(pathsave_commonid, outfile)])\n",
    "    else:\n",
    "        subprocess.run([\"plink\", \"--bfile\", filename, \"--keep\", keepfile, \"--make-bed\", \"--out\", os.path.join(pathsave_commonid, outfile)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lift Over\n",
    "\n",
    "Before the QC procedure, we will make sure to lift over the datasets from hg18 to hg19. We will use [LiftOverPlink](https://github.com/sritchie73/liftOverPlink) as a wraper for [liftOver](https://genome.sph.umich.edu/wiki/LiftOver). We will also [download](http://hgdownload.cse.ucsc.edu/goldenPath/hg18/liftOver/hg18ToHg19.over.chain.gz) the chain file that tells how to lift over from hg18 to hg19. This is only done for the CV, and Euro datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting MAP file to UCSC BED file...\n",
      "SUCC:  map->bed succ\n",
      "Lifting BED file...\n",
      "SUCC:  liftBed succ\n",
      "Converting lifted BED file back to MAP...\n",
      "SUCC:  bed->map succ\n",
      "cleaning up BED files...\n",
      "Converting MAP file to UCSC BED file...\n",
      "SUCC:  map->bed succ\n",
      "Lifting BED file...\n",
      "SUCC:  liftBed succ\n",
      "Converting lifted BED file back to MAP...\n",
      "SUCC:  bed->map succ\n",
      "cleaning up BED files...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(pathgenos, \"01_Original\", \"Extract_Common_ID\"))\n",
    "liftoverplink = os.path.join(projpath, \"Code\", \"liftOverPlink\")\n",
    "badlifts      = os.path.join(projpath, \"Code\", \"rmBadLifts\")\n",
    "chainfile     = os.path.join(pathdat, \"hg18ToHg19.over.chain.gz\")\n",
    "for mapfile in glob.glob(\"*.map\"):\n",
    "    if \"CV_\" in mapfile or \"Euro180_\" in mapfile:\n",
    "        outlifted    = mapfile.split(\".\")[0] + \"_lifted\"\n",
    "        loglifted    = mapfile.split(\".\")[0] + \"_bad_lifted.dat\"\n",
    "        \n",
    "        %run $liftoverplink --map $mapfile --out lifted --chain $chainfile\n",
    "        %run $badlifts --map lifted.map --out good_lifted.map --log $loglifted\n",
    "        \n",
    "        #Creating a list of snps to include in lifted version\n",
    "        snps = pd.read_csv(\"good_lifted.map\", sep = \"\\t\", header = None)\n",
    "        snps.iloc[:,1].to_csv(\"snplist.txt\", index = False)\n",
    "        \n",
    "        #Excluding snps and creating binary file\n",
    "        subprocess.run([\"plink\", \"--file\", mapfile.split(\".\")[0], \"--recode\", \"--out\", \"lifted\", \"--extract\", \"snplist.txt\" ])\n",
    "        subprocess.run([\"plink\", \"--file\", \"--ped\", \"lifted.ped\", \"--map\", \"good_lifted.map\", \"--make-bed\", \"--out\", outlifted])\n",
    "        \n",
    "        to_remove = [\"lifted.ped\", \"lifted.map\", \"good_lifted.map\", \"snplist.txt\"]\n",
    "        for file in to_remove:\n",
    "            os.remove(file)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing some files\n",
    "for file in glob.glob(\"*.ped\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "for file in glob.glob(\"*.map\"):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC procedure\n",
    "\n",
    "The QC procedure runs as follows:\n",
    "\n",
    "1. Removed founders, that is, individuals with at least one parent in the dataset, and retained only autosomal chromosomes\n",
    "2. Removed SNPs with missing call rates higher than 0.1\n",
    "3. Removed SNPs with minor allele frequencies below 0.05\n",
    "4. Removed SNPs with hardy-weinberg equilibrium p-values less than 1e-50\n",
    "5. Removed samples with missing call rates higher than 0.1\n",
    "6. Removed one arbitrary individual from any pairwise comparison with a pihat >= 0.25 from an IBD estimation after LD prune\n",
    "\n",
    "QC in all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Euro180_176ppl_317K_hg19_ATGC_CID_lifted file...\n",
      "Removing SNPs with missing call rates higher than 0.1...\n",
      "Removing SNPs with minor allele frequencies below 0.05...\n",
      "Removing SNPs with hardy-weinberg equilibrium p-values less than 1e-50...\n",
      "Removing samples with missing call rates higher than 0.1...\n",
      "Removing one arbitrary individual from any pairwise comparison with a pihat higher than 0.25...\n",
      "Generating final plink file...\n",
      "Running ADAPT_2784ppl_567K_hg19_CID file...\n",
      "Removing SNPs with missing call rates higher than 0.1...\n",
      "Removing SNPs with minor allele frequencies below 0.05...\n",
      "Removing SNPs with hardy-weinberg equilibrium p-values less than 1e-50...\n",
      "Removing samples with missing call rates higher than 0.1...\n",
      "Removing one arbitrary individual from any pairwise comparison with a pihat higher than 0.25...\n",
      "Generating final plink file...\n",
      "Running CV_697ppl_964K_hg19_ATGC_CID_lifted file...\n",
      "Removing SNPs with missing call rates higher than 0.1...\n",
      "Removing SNPs with minor allele frequencies below 0.05...\n",
      "Removing SNPs with hardy-weinberg equilibrium p-values less than 1e-50...\n",
      "Removing samples with missing call rates higher than 0.1...\n",
      "Removing one arbitrary individual from any pairwise comparison with a pihat higher than 0.25...\n",
      "Generating final plink file...\n",
      "Running SA_231ppl_599K_hg19_ATGC_CID file...\n",
      "Removing SNPs with missing call rates higher than 0.1...\n",
      "Removing SNPs with minor allele frequencies below 0.05...\n",
      "Removing SNPs with hardy-weinberg equilibrium p-values less than 1e-50...\n",
      "Removing samples with missing call rates higher than 0.1...\n",
      "Removing one arbitrary individual from any pairwise comparison with a pihat higher than 0.25...\n",
      "Generating final plink file...\n",
      "Running UIUC2013_116ppl_959K_hg19_ATGC_CID file...\n",
      "Removing SNPs with missing call rates higher than 0.1...\n",
      "Removing SNPs with minor allele frequencies below 0.05...\n",
      "Removing SNPs with hardy-weinberg equilibrium p-values less than 1e-50...\n",
      "Removing samples with missing call rates higher than 0.1...\n",
      "Removing one arbitrary individual from any pairwise comparison with a pihat higher than 0.25...\n",
      "Generating final plink file...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Move to directory and run QC procedure\n",
    "os.chdir(os.path.join(pathgenos, \"01_Original\", \"Extract_Common_ID\"))\n",
    "QC_procedure()\n",
    "\n",
    "#Remove no sex individuals in CHP (plates)\n",
    "#os.chdir(os.path.join(pathgenos, \"01_Original\", \"QC\"))\n",
    "#for file in glob.glob(\"CHP*.bed\"):\n",
    "#    filename1 = file.split(\".\")[0]\n",
    "#    subprocess.run([\"plink\", \"--bfile\", filename1, \"--remove\", filename1 + \".nosex\", \n",
    "#                        \"--make-bed\", \"--out\", filename1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC in _phenos subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving files to 02_Clean folder, and remove some intermediary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving files to 02_Clean folder\n",
    "os.chdir(os.path.join(pathgenos, \"01_Original\", \"Extract_Common_ID\", \"QC\"))\n",
    "for file in glob.glob(\"*_rel*\"):\n",
    "    shutil.move(file, os.path.join(outpath, file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us look at how many SNPs and individuals ended in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: ADAPT_2784ppl_567K_hg19_CID_founders_geno01_maf_hwe_mind01_rel.log\n",
      "463723 variants loaded from .bim file.\n",
      "463723 variants and 2289 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: SA_231ppl_599K_hg19_ATGC_CID_founders_geno01_maf_hwe_mind01_rel.log\n",
      "458402 variants loaded from .bim file.\n",
      "458402 variants and 30 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Euro180_176ppl_317K_hg19_ATGC_CID_lifted_founders_geno01_maf_hwe_mind01_rel.log\n",
      "295854 variants loaded from .bim file.\n",
      "295854 variants and 171 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: CV_697ppl_964K_hg19_ATGC_CID_lifted_founders_geno01_maf_hwe_mind01_rel.log\n",
      "790537 variants loaded from .bim file.\n",
      "790537 variants and 154 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: UIUC2013_116ppl_959K_hg19_ATGC_CID_founders_geno01_maf_hwe_mind01_rel.log\n",
      "774977 variants loaded from .bim file.\n",
      "774977 variants and 86 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(outpath)\n",
    "for file in glob.glob(\"*_rel.log\"):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file)\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

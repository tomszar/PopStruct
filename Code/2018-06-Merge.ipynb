{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging genotypes\n",
    "\n",
    "In this script we will take the harmonized genotypes and merge them.\n",
    "Note that UIUC2014 only contain duplicates from the ADAPT file, so we will remove it from further analysis.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Let's import modules and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from GenotypeQC import Split_chr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting paths\n",
    "projpath  = os.path.realpath(\"..\")\n",
    "pathgenos = os.path.join(projpath, \"DataBases\", \"Genotypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating each file by chromosome\n",
    "\n",
    "Let's first merge the different files divided by chromosomes into a single one.\n",
    "In the CV file we'll need to remove some duplicated SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move directory\n",
    "os.chdir(pathgenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering every directory in harmonize folder\n",
    "filenames = os.listdir(os.path.join(pathgenos, \"03_Harmonized\"))\n",
    "\n",
    "for filename in filenames: # loop through all the files and folders\n",
    "    if os.path.isdir(os.path.join(pathgenos, \"03_Harmonized\", filename)):\n",
    "        os.chdir(os.path.join(pathgenos, \"03_Harmonized\", filename))\n",
    "        #Opening new file and pasting the file names to use for merging\n",
    "        f = open(filename + \".txt\", \"w+\")\n",
    "        for file in glob.glob(\"*_harmonized.bed\"):\n",
    "            f.write(file.split(\".\")[0] + \"\\n\")\n",
    "        f.close()\n",
    "        if \"CV_\" in filename or \"Euro\" in filename: #in CV and Euro we need to remove some SNPs before merging\n",
    "            subprocess.run([\"plink\", \"--merge-list\", filename + \".txt\", \"--allow-no-sex\", \"--make-bed\", \"--out\", \n",
    "                            os.path.join(pathgenos, \"03_Harmonized\", filename + \"_excludedtemp\" )])\n",
    "            for file in glob.glob(\"*_harmonized.bed\"):\n",
    "                subprocess.run([\"plink\", \"--bfile\", file.split(\".\")[0], \"--exclude\", \n",
    "                                os.path.join(pathgenos, \"03_Harmonized\", filename + \"_excludedtemp-merge.missnp\" ), \n",
    "                                \"--make-bed\", \"--out\", file.split(\".\")[0] + \"_temp\"])\n",
    "                \n",
    "            f = open(filename + \".txt\", \"w+\")\n",
    "            for file in glob.glob(\"*_temp.bed\"):\n",
    "                f.write(file.split(\".\")[0] + \"\\n\")\n",
    "            f.close()\n",
    "            subprocess.run([\"plink\", \"--merge-list\", filename + \".txt\", \"--allow-no-sex\", \"--make-bed\", \"--out\", \n",
    "                            os.path.join(pathgenos, \"03_Harmonized\", filename + \"_all_harmonized\" )])\n",
    "            for file in glob.glob(\"*_temp.*\"):\n",
    "                os.remove(file)   \n",
    "                \n",
    "        else:\n",
    "            subprocess.run([\"plink\", \"--merge-list\", filename + \".txt\", \"--allow-no-sex\", \"--make-bed\", \"--out\", \n",
    "                            os.path.join(pathgenos, \"03_Harmonized\", filename + \"_all_harmonized\" )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and cleaning all files\n",
    "\n",
    "Now we will merge the harmonized files into four different dataset as explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move directory\n",
    "os.chdir(os.path.join(pathgenos, \"03_Harmonized\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating initial merging files without UIUC2014\n",
    "f = open(\"merge_sparse.txt\", \"w+\")\n",
    "for file in glob.glob(\"*harmonized.bed\"):\n",
    "    if \"CHP\" in file or \"UIUC2014\" in file:\n",
    "        pass\n",
    "    else:\n",
    "        f.write(file.split(\".\")[0] + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all listed files, create SNP list to exclude and exclude them from all files to merge\n",
    "\n",
    "#For each file with the list of datasets create a file of excluded SNPs\n",
    "for setfile in glob.glob(\"merge*.txt\"): \n",
    "    setname   = setfile.split(\".\")[0]\n",
    "    #reading file to get the database names\n",
    "    filenames = pd.read_csv(setfile, header = None) \n",
    "    subprocess.run([\"plink\", \"--merge-list\", setfile, \"--make-bed\", \"--out\", os.path.join(pathgenos, \"04_Merge\", setname, \"ExcludeSnps\") ])\n",
    "    \n",
    "    for i in range( (len(filenames.index)) ):\n",
    "        #excluding SNPs from each database on the list\n",
    "        subprocess.run([\"plink\", \"--bfile\", filenames.iloc[i,0], \"--exclude\", os.path.join(pathgenos, \"04_Merge\", setname, \"ExcludeSnps-merge.missnp\"),\n",
    "                        \"--make-bed\", \"--out\", os.path.join(pathgenos, \"04_Merge\", setname, filenames.iloc[i,0] + \"_excludedtemp\") ])\n",
    "    #creating a new mergefile with the databases with excluded SNPs\n",
    "    f = open(os.path.join(pathgenos, \"04_Merge\", setname, \"mergefile.txt\"), \"w+\") \n",
    "    for bedfile in glob.glob(os.path.join(pathgenos, \"04_Merge\", setname, \"*.bed\") ):\n",
    "        f.write(bedfile.split(\".\")[0] + \"\\n\")\n",
    "    f.close()\n",
    "    #Second merging for all phenos file and QC process\n",
    "    subprocess.run([\"plink\", \"--merge-list\", os.path.join(pathgenos, \"04_Merge\", setname, \"mergefile.txt\"), \n",
    "                    \"--make-bed\", \"--out\", os.path.join(pathgenos, \"04_Merge\", setname, setname) ])\n",
    "\n",
    "    subprocess.run([\"plink\", \"--bfile\", os.path.join(pathgenos, \"04_Merge\", setname, setname), #Being more strict (--geno 0.08) in here allows to retain the CV samples\n",
    "                    \"--geno\", \"0.05\", \"--make-bed\", \"--out\", os.path.join(pathgenos, \"04_Merge\", setname, setname + \"_geno\") ]) \n",
    "\n",
    "    subprocess.run([\"plink\", \"--bfile\", os.path.join(pathgenos, \"04_Merge\", setname, setname + \"_geno\"), \n",
    "                    \"--maf\", \"0.05\", \"--make-bed\", \"--out\", os.path.join(pathgenos, \"04_Merge\", setname, setname + \"_geno\" + \"_maf\")])\n",
    "\n",
    "    subprocess.run([\"plink\", \"--bfile\", os.path.join(pathgenos, \"04_Merge\", setname, setname + \"_geno\" + \"_maf\"), \n",
    "                    \"--hwe\", \"1e-50\", \"--make-bed\", \"--out\", os.path.join(pathgenos, \"04_Merge\", setname, setname + \"_geno\" + \"_maf\" + \"_hwe\") ])\n",
    "   \n",
    "    subprocess.run([\"plink\", \"--bfile\", os.path.join(pathgenos, \"04_Merge\", setname, setname + \"_geno\" + \"_maf\" + \"_hwe\"), \n",
    "                    \"--mind\", \"0.05\", \"--make-bed\", \"--out\", os.path.join(pathgenos, \"04_Merge\", setname, setname + \"_geno\" + \"_maf\" + \"_hwe\" + \"_mind01\")])\n",
    "    \n",
    "    for file in glob.glob(os.path.join(pathgenos, \"04_Merge\", setname, \"*\") ): #Removing intermediary files\n",
    "        exclude = [\"mind01.\", \"split\"]\n",
    "        if any(x in file for x in exclude):\n",
    "            pass\n",
    "        else:\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many SNPs and samples per file there are.\n",
    "There are no duplicated IIDs left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: /home/tomas/Documents/Research/PopStruct/DataBases/Genotypes/04_Merge/merge_sparse/merge_sparse_geno_maf_hwe_mind01\n",
      "126937 variants loaded from .bim file.\n",
      "2730 people (942 males, 1788 females) loaded from .fam.\n",
      "126937 variants and 2729 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(os.path.join(pathgenos, \"04_Merge\", \"**\", \"*.log\") ):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file.split(\".\")[0])\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"people\" in line or \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the database in chromosomes to use fineStructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(pathgenos, \"04_Merge\", \"merge_sparse\"))\n",
    "Split_chr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with reference samples and LD prune\n",
    "Now we will merge the merged dataset with the reference samples, and run LD pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(pathgenos, \"04_Merge\"))\n",
    "reference_geno = glob.glob(os.path.join(pathgenos, \"00_Reference\", \"*.bed\")) #Location of reference genomes\n",
    "ref_file       = reference_geno[0].split(\".\")[0]\n",
    "ref_slice      = reference_geno[0].split(\".\")[0] + \"_slice\"\n",
    "\n",
    "#Entering every directory in merge folder\n",
    "filenames = os.listdir(os.chdir(os.path.join(pathgenos, \"04_Merge\")))\n",
    "\n",
    "for filename in filenames: \n",
    "    #If filename is a folder enter in it\n",
    "    if os.path.isdir(os.path.join(pathgenos, \"04_Merge\", filename)):\n",
    "        os.chdir(os.path.join(pathgenos, \"04_Merge\", filename))\n",
    "        for file in glob.glob(\"*.bim\"):\n",
    "            snps = pd.read_csv(file, sep = \"\\t\", header = None)\n",
    "            snps.iloc[:,1].to_csv(\"snplist.txt\", index = False)\n",
    "            \n",
    "        #For each bed file, generate a snplist and extract them from the reference, and then merge them\n",
    "        for file in glob.glob(\"*.bed\"): \n",
    "            file_pref = file.split(\".\")[0]\n",
    "            subprocess.run([\"plink\", \"--bfile\", ref_file, \"--extract\", \"snplist.txt\", \"--make-bed\", \"--out\", ref_slice])\n",
    "            #First merging will create a list of problematic SNPs\n",
    "            subprocess.run([\"plink\", \"--bfile\", file_pref, \"--bmerge\", ref_slice, \"--make-bed\", \"--out\", file_pref + \"_ref\"]) \n",
    "            \n",
    "            #Removing problematic SNPs from datasets and try a merging again\n",
    "            removesnp = glob.glob(\"*.missnp\")[0]\n",
    "            subprocess.run([\"plink\", \"--bfile\", file_pref, \"--exclude\", removesnp , \"--make-bed\", \"--out\", file_pref + \"_temp\" ])\n",
    "            \n",
    "            subprocess.run([\"plink\", \"--bfile\", ref_slice, \"--exclude\", removesnp, \"--make-bed\", \"--out\", ref_slice + \"_temp\" ])\n",
    "            \n",
    "            #Merging again\n",
    "            ref_output     = os.path.join(pathgenos, \"05_Ref\", file_pref + \"_ref\") #Output location\n",
    "            subprocess.run([\"plink\", \"--bfile\", file_pref + \"_temp\", \"--bmerge\", ref_slice + \"_temp\", \"--make-bed\", \"--out\", ref_output ])\n",
    "            \n",
    "            #Basic QC\n",
    "            subprocess.run([\"plink\", \"--bfile\", ref_output, \"--geno\", \"0.1\", \"--make-bed\", \"--out\", ref_output ])\n",
    "            subprocess.run([\"plink\", \"--bfile\", ref_output, \"--maf\", \"0.05\", \"--make-bed\", \"--out\", ref_output ])\n",
    "            subprocess.run([\"plink\", \"--bfile\", ref_output, \"--mind\", \"0.1\", \"--make-bed\", \"--out\", ref_output ])\n",
    "\n",
    "            #LD prune\n",
    "            ld_output      = os.path.join(pathgenos, \"06_Pruned\", file_pref + \"_ref\" + \"_pruned\" ) #Output location\n",
    "            subprocess.run([\"plink\", \"--bfile\", ref_output, \"--indep-pairwise\", \"50\", \"5\", \"0.5\", \"--make-bed\", \"--out\", ld_output])\n",
    "            subprocess.run([\"plink\", \"--bfile\", ref_output, \"--exclude\", ld_output + \".prune.out\", \"--make-bed\", \"--out\", ld_output])\n",
    "\n",
    "            #Remove intermediate files\n",
    "            for file in glob.glob(\"*_temp*\"):\n",
    "                os.remove(file)\n",
    "            for file in glob.glob(\"*_ref*\"):\n",
    "                os.remove(file)\n",
    "            for file in glob.glob(os.path.join(pathgenos, \"00_Reference\", \"*_slice*\" )):\n",
    "                os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many SNPs after the merging with the reference sampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: /home/tomas/Documents/Research/PopStruct/DataBases/Genotypes/05_Ref/merge_sparse_geno_maf_hwe_mind01_ref\n",
      "96780 variants loaded from .bim file.\n",
      "6173 people (2793 males, 3380 females) loaded from .fam.\n",
      "0 people removed due to missing genotype data (--mind).\n",
      "96780 variants and 6173 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(os.path.join(pathgenos, \"05_Ref\", \"*.log\") ):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file.split(\".\")[0])\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"people\" in line or \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after LD prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: /home/tomas/Documents/Research/PopStruct/DataBases/Genotypes/06_Pruned/merge_sparse_geno_maf_hwe_mind01_ref_pruned\n",
      "96780 variants loaded from .bim file.\n",
      "6173 people (2793 males, 3380 females) loaded from .fam.\n",
      "--exclude: 85536 variants remaining.\n",
      "85536 variants and 6173 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(os.path.join(pathgenos, \"06_Pruned\", \"*.log\") ):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file.split(\".\")[0])\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"people\" in line or \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split by chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(pathgenos, \"05_Ref\"))\n",
    "Split_chr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the reference from our samples (to generate the CV estimation with admixture first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(os.path.join(pathgenos, \"06_Pruned\", \"*.bed\") ):\n",
    "    filename = file.split(\".\")[0]\n",
    "    refpops  = glob.glob(os.path.join(pathgenos, \"00_Reference\", \"*.fam\") )[0]\n",
    "    subprocess.run([\"plink\", \"--bfile\", filename, \"--remove\", refpops, \"--make-bed\", \"--out\", filename + \"_splitadapt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA from pruned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running PCA from pruned files\n",
    "os.chdir(os.path.join(pathgenos, \"06_Pruned\") )\n",
    "pathpca = os.path.join(projpath, \"Results\", \"PCA\")\n",
    "for file in glob.glob(\"*pruned.bed\"):\n",
    "    filename = file.split(\".\")[0]\n",
    "    outname  = os.path.join(pathpca, filename + \"_PCA\")\n",
    "    subprocess.run([\"plink\", \"--bfile\", filename, \"--pca\", \"50\", \"--out\", outname])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
